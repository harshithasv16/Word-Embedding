# Word-Embedding

Word embedding is a learned representation for text where words that have the same meaning
have a similar representation.
They are an improvement over sparse representations used in simpler bag of words 
model representation.

Steps to follow:
1. Sentences
2. One hot representation -> index from dictionary
3. Onehot Repre -> embedding layer keras
4. Embedding matrix
